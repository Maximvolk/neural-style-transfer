{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural_style_transfer.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RtXrj2nJA0gm","colab_type":"text"},"source":["# Neural style transfer"]},{"cell_type":"markdown","metadata":{"id":"LvA1A2XSA4mx","colab_type":"text"},"source":["### Mount google drive with data"]},{"cell_type":"code","metadata":{"id":"XNtEyF94AqFF","colab_type":"code","outputId":"26be0780-53ed-4ef8-945b-8037b98a6038","executionInfo":{"status":"ok","timestamp":1557166166538,"user_tz":-180,"elapsed":24478,"user":{"displayName":"Maxim Volkov","photoUrl":"","userId":"17186248271287867220"}},"colab":{"base_uri":"https://localhost:8080/","height":169}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wTE9h0ZXBWMI","colab_type":"text"},"source":["### Load pretrained model and construct its custom version"]},{"cell_type":"code","metadata":{"id":"yDXGEBD3BAKe","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","tf.enable_eager_execution()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52BQk7ZaDXgv","colab_type":"text"},"source":["Prepare style and content layers' names"]},{"cell_type":"code","metadata":{"id":"Q15uymzPDcR1","colab_type":"code","colab":{}},"source":["style_layers = [\n","    'block1_conv1',\n","    'block2_conv1',\n","    'block3_conv1',\n","    'block4_conv1',\n","    'block5_conv1'\n","]\n","\n","content_layers = ['block4_conv2']\n","\n","n_content = len(content_layers)\n","n_style = len(style_layers)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FxC5O1_2Bi85","colab_type":"code","colab":{}},"source":["def get_model():\n","  \"\"\"\n","  Load VGG19 pre-trained on ImageNet (without fc layers) as\n","  a feature extractor.\n","  Return: custom model which returns extracted content and style features\n","  \"\"\"\n","  \n","  # Base network\n","  vgg = tf.keras.applications.vgg19.VGG19(include_top=False,\n","                                          weights='imagenet',\n","                                          input_shape=[512, 512, 3])\n","  vgg.trainable = False\n","  \n","  # Extract content and style features\n","  content_features = [vgg.get_layer(l).output for l in content_layers]\n","  style_features = [vgg.get_layer(l).output for l in style_layers]\n","  model_output = content_features + style_features\n","  \n","  return tf.keras.models.Model(vgg.input, model_output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQPvPWPpLpD0","colab_type":"text"},"source":["### Image preprocessing"]},{"cell_type":"code","metadata":{"id":"EX2IeZrRNmo0","colab_type":"code","colab":{}},"source":["def get_and_preprocess_image(image_path):\n","  # Read image and transform it to tensor\n","  image = tf.read_file(image_path)\n","  image = tf.image.decode_image(image)\n","  image = tf.image.resize_images(image, [512, 512])\n","  \n","  image = tf.cast(image, 'float32')\n","  \n","  # Add batch dimension\n","  image = tf.expand_dims(image, 0)\n","  \n","  return image"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4PYCSXWdayct","colab_type":"text"},"source":["The result image (with style transfered from another image) will have the same shape as input image. So, style image will be resized to the size of input image."]},{"cell_type":"markdown","metadata":{"id":"u0FlS-tZbjPm","colab_type":"text"},"source":["### Feature extraction"]},{"cell_type":"code","metadata":{"id":"0QO8xzvRETQ6","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","\n","def get_features(model, input_image_path, style_image_path):\n","  \"\"\"  \n","  Return: content and style features as 2 tensors\n","  \"\"\"\n","  \n","  content_image = get_and_preprocess_image(input_image_path)\n","  style_image = get_and_preprocess_image(style_image_path)\n","  \n","  # Concatenate images to pass them through the model at once\n","  images = np.concatenate([content_image, style_image], axis=0)\n","  model_outputs = model(images)\n","  \n","  content_features = [layer[0] for layer in model_outputs[:n_content]]\n","  style_features = [layer[1] for layer in model_outputs[n_content:]]\n","  \n","  return content_features, style_features"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2eNwdVnqhDXm","colab_type":"text"},"source":["### Loss"]},{"cell_type":"code","metadata":{"id":"gqvTXobuf7nL","colab_type":"code","colab":{}},"source":["# Content loss\n","def get_content_loss(acquired, target):\n","  return tf.reduce_mean(tf.square(acquired - target))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ukzBlAtci_sN","colab_type":"code","colab":{}},"source":["# Gram matrix\n","def get_gram_matrix(A):\n","  A = A - 1\n","  \n","  channels = int(A.shape[-1])\n","  a = tf.reshape(A, [-1, channels])\n","  n = tf.shape(a)[0]\n","  gram = tf.matmul(a, a, transpose_a=True)\n","  return gram / tf.cast(n, tf.float32)\n","\n","# Style loss\n","def get_style_loss(acquired, target):\n","  A = get_gram_matrix(acquired)\n","  G = get_gram_matrix(target)\n","  N = tf.keras.backend.int_shape(acquired)[0]\n","  M = tf.keras.backend.int_shape(acquired)[1]\n","  \n","  return tf.reduce_mean(tf.square(A - G))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dwTzVITAfJf_","colab_type":"code","colab":{}},"source":["def compute_loss(model, loss_weights, init_image, target_style, target_content):\n","  \"\"\"\n","  Compute total loss between:\n","      content/style of init_image and content/style of target image\n","      (target_style, target_content)\n","  \"\"\"\n","  \n","  # Unpack loss weights\n","  style_weight, content_weight = loss_weights\n","  \n","  # Initialize losses\n","  style_loss = 0\n","  content_loss = 0\n","  \n","  # Pass init_image through model\n","  model_outputs = model(init_image)\n","  init_content = model_outputs[:n_content]\n","  init_style = model_outputs[n_content:]\n","  \n","  # Compute style loss\n","  style_layer_weight = 1 / n_style\n","  for style, t_style in zip(init_style, target_style):\n","    style_loss += style_layer_weight*get_style_loss(style, t_style)\n","    \n","  # Compute content loss\n","  content_layer_weight = 1 / n_content\n","  for content, t_content in zip(init_content, target_content):\n","    content_loss += content_layer_weight*get_content_loss(content, t_content)\n","    \n","  style_loss *= style_weight\n","  content_loss *= content_weight\n","  total_loss = style_loss + content_loss\n","  \n","  return total_loss, style_loss, content_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXTBlHV3lC0u","colab_type":"text"},"source":["### Gradients"]},{"cell_type":"code","metadata":{"id":"twmGSpwpk-16","colab_type":"code","colab":{}},"source":["def compute_gradients(cfg):\n","  with tf.GradientTape() as tape:\n","    loss = compute_loss(**cfg)\n","  \n","  total_loss = loss[0]\n","  return tape.gradient(total_loss, cfg['init_image']), loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PJ1eI-snlg7o","colab_type":"text"},"source":["### Run training"]},{"cell_type":"code","metadata":{"id":"Cq6gmHsb1JTf","colab_type":"code","colab":{}},"source":["def deprocess_image(image):\n","  \"\"\"\n","  Helper function to deprocess tf.Variable to image\n","  \n","  Args:\n","    image: tf.Variable\n","    \n","  Returns:\n","    png image (array of bytes)\n","  \"\"\"\n","  \n","  arr = image.numpy()\n","  arr = tf.squeeze(arr, axis=0).numpy()\n","  img = tf.image.encode_png(tf.cast(arr, tf.uint8)).numpy()\n","  \n","  return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f5Jcf4Ino6tF","colab_type":"code","colab":{}},"source":["model = get_model()\n","\n","for layer in model.layers:\n","  layer.trainable = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5BtS2BenleaE","colab_type":"code","colab":{}},"source":["# Load images\n","GDRIVE = '/content/gdrive/My Drive/neural_style_transfer/'\n","CONTENT_PATH = GDRIVE + 'content/2.jpg'\n","STYLE_PATH = GDRIVE + 'styles/scream.jpg'\n","INIT_PATH = GDRIVE + 'content/2.jpg'\n","init_image = get_and_preprocess_image(INIT_PATH)\n","init_image = tf.Variable(init_image, dtype=tf.float32)\n","\n","# Extract target features\n","content_features_target, style_features_target = get_features(model, CONTENT_PATH, STYLE_PATH)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQb2g2WhpuA9","colab_type":"code","colab":{}},"source":["num_iterations = 1000\n","\n","# For displaying\n","display_num = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SkL2LT6Eq9GX","colab_type":"code","colab":{}},"source":["best_loss = float('inf')\n","best_img = None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZihUo_drz1g","colab_type":"code","colab":{}},"source":["optimizer = tf.train.AdamOptimizer(learning_rate=8.0, beta1=0.99, epsilon=1e-1)\n","loss_weights = (1e3, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UYvCqtvysM9C","colab_type":"code","colab":{}},"source":["# Config for gradients computing\n","cfg = {\n","    'model': model,\n","    'loss_weights': loss_weights,\n","    'init_image': init_image,\n","    'target_style': style_features_target,\n","    'target_content': content_features_target\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Byk7k5U9ZUa","colab_type":"code","colab":{}},"source":["import IPython.display as display\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gh4aNIy8vXVx","colab_type":"code","outputId":"a59cc8af-9945-442d-cbb3-707ae0800628","colab":{"base_uri":"https://localhost:8080/","height":5477,"output_embedded_package_id":"1nU7rnA-BF7mCrLjmtIlGBfVzVI5cJFnr"},"executionInfo":{"status":"ok","timestamp":1557171315554,"user_tz":-180,"elapsed":152657,"user":{"displayName":"Maxim Volkov","photoUrl":"","userId":"17186248271287867220"}}},"source":["# Values clipping\n","norm_means = np.array([103.939, 116.779, 123.68])\n","min_vals = -norm_means\n","max_vals = 255 - norm_means\n","\n","\n","# Training\n","time_init = time.time()\n","for i in range(num_iterations):\n","  grads, loss = compute_gradients(cfg)\n","  total_loss, style_loss, content_loss = loss\n","  optimizer.apply_gradients([(grads, init_image)])\n","  \n","  # Clipping\n","  clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n","  init_image.assign(clipped)\n","  \n","  # Update loss and image\n","  if total_loss < best_loss:\n","    best_loss = total_loss\n","    best_img = init_image\n","  \n","  # Display every 100th iteration result\n","  if i % display_num == 0:\n","    time_100 = time.time()\n","    image_out = deprocess_image(init_image)\n","    \n","    print('Iteration: {}\\nTime spent: {} min'.format(i, (time_100 - time_init)/60))\n","    display.display(display.Image(image_out))"],"execution_count":130,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"wYdkk81P-00E","colab_type":"code","colab":{}},"source":["# Save best image to gdrive\n","best_image = deprocess_image(best_img)\n","\n","with open(GDRIVE + 'output/2+scream.png', 'wb') as f:\n","  f.write(best_image)"],"execution_count":0,"outputs":[]}]}